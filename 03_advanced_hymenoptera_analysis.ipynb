{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hymenoptera 데이터셋에 대한 고급 이미지 분류 실험\n",
        "\n",
        "# 1. 필요한 라이브러리 임포트 및 환경 설정\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, models, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import timm\n",
        "\n",
        "# Google Drive 마운트 및 기본 설정\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 필요한 패키지 설치\n",
        "%pip install torch torchvision tqdm pandas matplotlib timm\n",
        "\n",
        "# 프로젝트 디렉토리 설정\n",
        "project_path = '/content/drive/MyDrive/Final'\n",
        "os.chdir(project_path)\n",
        "\n",
        "# 디렉토리 구조 설정\n",
        "dirs = {\n",
        "    'model': os.path.join(project_path, 'model'),\n",
        "    'result': os.path.join(project_path, 'result'),\n",
        "    'plots': os.path.join(project_path, 'plots'),\n",
        "    'data': {\n",
        "        'train': os.path.join(project_path, 'hymenoptera_data/train'),\n",
        "        'val': os.path.join(project_path, 'hymenoptera_data/val')\n",
        "    }\n",
        "}\n",
        "\n",
        "# 필요한 디렉토리 생성\n",
        "for dir_path in [dirs['model'], dirs['result'], dirs['plots']]:\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "# 장치 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 2. 데이터 전처리 및 증강 설정\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Mixup 데이터 증강\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# 데이터셋 로드\n",
        "train_dataset = datasets.ImageFolder(dirs['data']['train'], transform=data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder(dirs['data']['val'], transform=data_transforms['val'])\n",
        "\n",
        "# 클래스 정보 출력\n",
        "print(\"\\n데이터셋 정보:\")\n",
        "print(\"클래스:\", train_dataset.classes)\n",
        "print(\"클래스 수:\", len(train_dataset.classes))\n",
        "print(\"학습 데이터 수:\", len(train_dataset))\n",
        "print(\"검증 데이터 수:\", len(val_dataset))\n",
        "\n",
        "# 데이터로더 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# 3. 모델 설정 및 학습\n",
        "def setup_model(model_name, num_classes):\n",
        "    \"\"\"모델 설정\"\"\"\n",
        "    if model_name.startswith('efficientnet'):\n",
        "        model = timm.create_model(model_name, pretrained=True)\n",
        "        model.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(model.classifier.in_features, num_classes)\n",
        "        )\n",
        "    elif model_name == 'vit':\n",
        "        model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        model.head = nn.Linear(model.head.in_features, num_classes)\n",
        "    else:\n",
        "        raise ValueError(f'Unknown model: {model_name}')\n",
        "    \n",
        "    return model.to(device)\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, model_name, num_epochs=20, mixup=True):\n",
        "    \"\"\"모델 학습 함수\"\"\"\n",
        "    best_acc = 0.0\n",
        "    results = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "        \n",
        "        # 학습 단계\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        \n",
        "        for inputs, labels in tqdm(train_loader, desc='Training'):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            if mixup:\n",
        "                inputs, labels_a, labels_b, lam = mixup_data(inputs, labels)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            \n",
        "            if mixup:\n",
        "                loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            if not mixup:\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "        \n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        \n",
        "        epoch_loss = running_loss / len(train_dataset)\n",
        "        epoch_acc = running_corrects.double() / len(train_dataset) if not mixup else 0.0\n",
        "        results['train_loss'].append(epoch_loss)\n",
        "        results['train_acc'].append(epoch_acc.item() if not mixup else 0.0)\n",
        "        \n",
        "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "        \n",
        "        # 검증 단계\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(val_loader, desc='Validation'):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "        \n",
        "        epoch_loss = running_loss / len(val_dataset)\n",
        "        epoch_acc = running_corrects.double() / len(val_dataset)\n",
        "        results['val_loss'].append(epoch_loss)\n",
        "        results['val_acc'].append(epoch_acc.item())\n",
        "        \n",
        "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "        \n",
        "        # 최고 성능 모델 저장\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': epoch_loss,\n",
        "                'acc': epoch_acc,\n",
        "            }, os.path.join(dirs['model'], f'{model_name}_best_{timestamp}.pt'))\n",
        "    \n",
        "    # 최종 결과 저장\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    pd.DataFrame(results).to_csv(os.path.join(dirs['result'], f'{model_name}_results_{timestamp}.csv'))\n",
        "    \n",
        "    return results\n",
        "\n",
        "# 4. 모델 실험 실행\n",
        "models_to_test = [\n",
        "    'efficientnet_b0',\n",
        "    'efficientnet_b3',\n",
        "    'vit'\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name in models_to_test:\n",
        "    print(f\"\\n{model_name} 모델 학습 시작\")\n",
        "    model = setup_model(model_name, len(train_dataset.classes))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "    \n",
        "    model_results = train_model(\n",
        "        model,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        model_name,\n",
        "        num_epochs=20,\n",
        "        mixup=True\n",
        "    )\n",
        "    \n",
        "    results[model_name] = model_results\n",
        "\n",
        "# 5. 결과 시각화\n",
        "def plot_training_results(results):\n",
        "    \"\"\"학습 결과 시각화\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    for model_name, history in results.items():\n",
        "        plt.plot(history['val_acc'], label=model_name)\n",
        "    plt.title('검증 정확도')\n",
        "    plt.xlabel('에폭')\n",
        "    plt.ylabel('정확도')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    for model_name, history in results.items():\n",
        "        plt.plot(history['val_loss'], label=model_name)\n",
        "    plt.title('검증 손실')\n",
        "    plt.xlabel('에폭')\n",
        "    plt.ylabel('손실')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(dirs['plots'], f'training_results_{timestamp}.png'))\n",
        "    plt.show()\n",
        "\n",
        "# 결과 시각화 실행\n",
        "plot_training_results(results)\n",
        "\n",
        "# 최종 결과 출력\n",
        "print(\"\\n=== 최종 결과 ===\")\n",
        "for model_name, history in results.items():\n",
        "    final_acc = history['val_acc'][-1]\n",
        "    print(f\"{model_name}: 최종 검증 정확도 = {final_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
